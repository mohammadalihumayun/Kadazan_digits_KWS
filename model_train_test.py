# -*- coding: utf-8 -*-
"""kadazan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EnnMLukm_b-eX3EWOwLfoWHQTRy58EwY
"""

import numpy as np
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, PReLU, Input, BatchNormalization,Conv1D, Flatten, MaxPooling1D, concatenate, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l1,l2,l1_l2
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_recall_fscore_support as prf
import csv
import random
import tracemalloc
import time

arraypath='/PATH_TO_MFCC_FEATURES_ARRAY'

#kdzdgtmfc=np.load(arraypath+'spk50_mfclen32.npy')[:,:13,:]
kdzdgtmfc=np.load(arraypath+'spk50_mfcpad115.npy')[:,:13,:]
kdzdgtmfc=np.swapaxes(kdzdgtmfc,1,2)
kdzdgtlbl=np.array(np.arange(len(kdzdgtmfc))%10)

def bmcnn(trainmfcc,trainlabelids,testmfcc,testlabelids,pt,epc):
 inputdim1=trainmfcc.shape
 outputdim1=to_categorical(trainlabelids).shape[1]
 input_flat1 = Input(shape=(inputdim1[1:]))
 h=input_flat1
 for layers in range(3):
  h=Conv1D(64, kernel_size=16,strides=1, padding='same')(h)#
  #h=Activation('relu')(h)
  h=BatchNormalization()(h)
  h=MaxPooling1D(4)(h)# 
 h=Flatten()(h)
 #h=Dropout(0.1)(h)
 #h=PReLU()(h)
 output_layer1 = Dense(outputdim1, activation='softmax')(h)
 emodel1 = Model(input_flat1,output_layer1)
 emodel1.compile(optimizer='Adamax', loss='categorical_crossentropy')
 emodel1.summary()
 es = EarlyStopping(monitor='loss', mode='min',verbose=1,min_delta=0, patience=pt)
 tracemalloc.start()
 start_time = time.time()
 hist= emodel1.fit(trainmfcc,to_categorical(trainlabelids),epochs=epc,batch_size=32,shuffle=True,callbacks=[es],verbose=1) 
 timetaken = time.time() - start_time
 current, peak = tracemalloc.get_traced_memory()
 pkmem=peak / 10**6
 tracemalloc.stop()
 prediction=emodel1.predict(testmfcc)
 acc=sum(np.argmax(prediction,axis=1)==testlabelids)/len(prediction)
 precrec=prf(testlabelids,np.argmax(prediction,axis=1))
 conf=cm(testlabelids,np.argmax(prediction,axis=1))
 return acc, conf, precrec,timetaken,pkmem

def dnncomp(trainmfcc,trainlabelids,testmfcc,testlabelids,pt,epc):
 inputdim1=trainmfcc.shape
 outputdim1=to_categorical(trainlabelids).shape[1]
 input_flat1 = Input(shape=(inputdim1[1:]))
 h=Flatten()(input_flat1)
 for layers in range(3):
  h=Dense(64)(h)#
  #h=Activation('relu')(h)
  h=BatchNormalization()(h)
 #h=Dropout(0.1)(h)
 #h=PReLU()(h)
 output_layer1 = Dense(outputdim1, activation='softmax')(h)
 emodel1 = Model(input_flat1,output_layer1)
 emodel1.compile(optimizer='Adamax', loss='categorical_crossentropy')
 emodel1.summary()
 es = EarlyStopping(monitor='loss', mode='min',verbose=1,min_delta=0, patience=pt)
 tracemalloc.start()
 start_time = time.time()
 hist= emodel1.fit(trainmfcc,to_categorical(trainlabelids),epochs=epc,batch_size=32,shuffle=True,callbacks=[es],verbose=1) 
 timetaken = time.time() - start_time
 current, peak = tracemalloc.get_traced_memory()
 pkmem=peak / 10**6
 tracemalloc.stop()
 prediction=emodel1.predict(testmfcc)
 acc=sum(np.argmax(prediction,axis=1)==testlabelids)/len(prediction)
 precrec=prf(testlabelids,np.argmax(prediction,axis=1))
 conf=cm(testlabelids,np.argmax(prediction,axis=1))
 return acc, conf, precrec,timetaken,pkmem

def rnncomp(trainmfcc,trainlabelids,testmfcc,testlabelids,pt,epc):
 inputdim1=trainmfcc.shape
 outputdim1=to_categorical(trainlabelids).shape[1]
 input_flat1 = Input(shape=(inputdim1[1:]))
 h=input_flat1
 for layers in range(2):
  h=LSTM(64,return_sequences=True)(h)  # LSTM for arbitrary length series.
  #h=Activation('relu')(h)
  h=BatchNormalization()(h)
 h=LSTM(64,return_sequences=False)(h)  # LSTM for arbitrary length series.
 #h=Dropout(0.1)(h)
 #h=PReLU()(h)
 output_layer1 = Dense(outputdim1, activation='softmax')(h)
 emodel1 = Model(input_flat1,output_layer1)
 emodel1.compile(optimizer='Adamax', loss='categorical_crossentropy')
 emodel1.summary()
 es = EarlyStopping(monitor='loss', mode='min',verbose=1,min_delta=0, patience=pt)
 tracemalloc.start()
 start_time = time.time()
 hist= emodel1.fit(trainmfcc,to_categorical(trainlabelids),epochs=epc,batch_size=32,shuffle=True,callbacks=[es],verbose=1) 
 timetaken = time.time() - start_time
 current, peak = tracemalloc.get_traced_memory()
 pkmem=peak / 10**6
 tracemalloc.stop()
 prediction=emodel1.predict(testmfcc)
 acc=sum(np.argmax(prediction,axis=1)==testlabelids)/len(prediction)
 precrec=prf(testlabelids,np.argmax(prediction,axis=1))
 conf=cm(testlabelids,np.argmax(prediction,axis=1))
 return acc, conf, precrec,timetaken,pkmem

pt=100
epc=25
ttldgt=len(kdzdgtmfc)
print(ttldgt)
print(int(ttldgt/20))
ntstspk=10
for itera in range(5):
  random.seed(itera)
  idxs=np.arange(ttldgt).reshape(int(ttldgt/10),10)
  tstspks=random.sample(list(np.arange(int(ttldgt/10))),ntstspk)
  trnspks=list(set(np.arange(int(ttldgt/10)))-set(tstspks))
  tstdgts=idxs[tstspks].flatten()
  trndgts=idxs[trnspks].flatten()
  #acc, conf, precrec,timetaken,pkmem=bmcnn(kdzdgtmfc[trndgts],kdzdgtlbl[trndgts],kdzdgtmfc[tstdgts],kdzdgtlbl[tstdgts],pt,epc)
  #acc, conf, precrec,timetaken,pkmem=dnncomp(kdzdgtmfc[trndgts],kdzdgtlbl[trndgts],kdzdgtmfc[tstdgts],kdzdgtlbl[tstdgts],pt,epc)
  #acc, conf, precrec,timetaken,pkmem=rnncomp(kdzdgtmfc[trndgts],kdzdgtlbl[trndgts],kdzdgtmfc[tstdgts],kdzdgtlbl[tstdgts],pt,epc)
  acc, conf, precrec,timetaken,pkmem=bmcnn(kdzdgtmfc[trndgts],kdzdgtlbl[trndgts],kdzdgtmfc[tstdgts],kdzdgtlbl[tstdgts],pt,epc)
  print('accuracy',acc)
  with open(arraypath+'kadazan_results_comparison_cpu_7july.csv', 'a', newline='') as csvfile:
     res = csv.writer(csvfile)
     #res.writerow(['itera','acc', 'conf', 'precrec','ntstspk',timetaken,pkmem])
     res.writerow([itera,acc, conf, precrec,ntstspk,timetaken,pkmem,'bmcnn'])

datasetfile=arraypath+'kadazan_results_13june.csv'
with open(datasetfile, newline='', encoding='ISO-8859-1') as f:
     reader = csv.reader(f)
     data = list(reader)

xarr=[]
for x in np.array(data)[:,3]:
    x1=x.replace(' ','').replace('[','').replace(']','').replace('(','').replace(')','').replace('\n','').split('array')[1:]
    x1narr=[]
    for x1n in x1:
        x1narr.append(np.array(x1n[:-1].split(',')).astype('float'))
    xarr.append(x1narr)

yarr=[]
for y in np.array(data)[1:,3]:
    y1=y.replace('[ ','').replace('[','').replace(']','').replace(' ',',').replace(',,',',').split('\n,')
    y1narr=[]
    for y1n in y1:
        y1narr.append(np.array(y1n.split(',')).astype('float'))
    yarr.append(y1narr)
np.array(yarr).shape

presi=np.mean(np.array(xarr)[:,0,:],axis=0)
recal=np.mean(np.array(xarr)[:,1,:],axis=0)
fscor=np.mean(np.array(xarr)[:,2,:],axis=0)
prf=[presi,recal,fscor]

np.array(data)[:,2]

lbl=['Precision','Recall','F_score']
for j in range(len(prf)):
 plt.plot(prf[j], label=lbl[j])
 
plt.xticks([0,1,2,3,4,5,6,7,8,9],['Iso', 'Duvo', 'Tohu', 'Apat', 'Himo', 'Onom', 'Tuu', 'Vahu', 'Sizam', 'Opod'] )
plt.legend()
plt.savefig('F:/kadazan_dataset/prf',dpi=300)
plt.show()

conf=np.sum(np.array(yarr),axis=0)

plt.figure(figsize=(7, 7))
plt.imshow(conf,cmap='YlGn')
plt.clim(0,np.max(conf))
plt.colorbar()
plt.xticks([0,1,2,3,4,5,6,7,8,9],['Iso', 'Duvo', 'Tohu', 'Apat', 'Himo', 'Onom', 'Tuu', 'Vahu', 'Sizam', 'Opod'] )
plt.yticks([0,1,2,3,4,5,6,7,8,9],['Iso', 'Duvo', 'Tohu', 'Apat', 'Himo', 'Onom', 'Tuu', 'Vahu', 'Sizam', 'Opod'] )

plt.savefig('F:/kadazan_dataset/conf',dpi=300)
plt.show()
